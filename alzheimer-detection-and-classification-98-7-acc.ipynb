{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Alzheimer Detection and Classifier\n\nThis notebook(codes) can be also downloaded from secret gist: https://tinyurl.com/2p8a9847\n\n## 1. Problem Definition\n> Alzheimer is known the most for the causes of dementia. It takes 2/3 of the whole dementia popilation, while the cause is still unknown. According to papers from Lancet neurology, even the old theory such as neural inflations, which was disregarded as the cause compared to tau-protein, beta-amiloids, and genetic factors (Apo E4). \n\nBefore I start, I'll have to explain a bit more about the Alzheimer, because, the data contains 4 classes of Alzheimer that depends on the severity of dementia. \n\nThere are degrees of severity in Alzheimer. \n1. Very mildly demented : This is the stage where patient starts to forget where they put their stuff, other people's names recently, etc. It is hard to detect through cognitive ability test. \n\n2. Mildly demented : This is the stage where patients don't remember the words, can't find their way to the destination, loss of focus and work-abilities. This is also the stage where patients even forget that they are losing memory. From this stage, with cognitive testing, it can be found. \n\n3. Moderately demented : Starts to forget the recent activities, important old histories, have hard time calculating the budget, hard to go outside alone, and loss of empathy. \n\nThere are 3 more stages in the moderately dementia, which in the terminal stage, the patient can't move on their own, while they lose the ability to speak. But I assume that the current dataset from Kaggle [https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset] is considering all the these stages merged inside 'Moderately demeted' or not even considered. \n\nKnowing these stages are important because the faster the stage the patient is at, the treatment will have higher effect in terms of slowing the process. If the dementia is found during the moderately demented stage, it is known that the patient will pass away in 3 years. (One of the reported case is a rythm guitarists Malcom from the band AC/DC was diagnosed severe dementia at 2014.)\n\nThus, having an AI that detects alzheimer dementia in the early stage can allow longer life expactancy from the patient as well as higher life quality overall from the slowdown of dementia. \n\nAs Alzheimer can not only be found with cognitive ability testing, but also through MRI or CT by looking at the ventricles of the brain and cortical atrophy, the theoratical foundation on this project is solid. Doctors find the patient with Alzheimer's have a brain that have enlarged ventricles (that lies in the center of the brain) as well as thinner cortical grey area of the brain.  ","metadata":{"id":"wtUL2uzglSrn"}},{"cell_type":"markdown","source":"## 2. Solution Specification\nThis project will go through various ML models from basic (Principle Component Analysis(PCA), Linear Discriminant Analysis(LDA), Support Vector Machine with 3 different kernels starting with linear to rbf kernel, and finally Convolutional Neural Network models VGG16 for Detection and EfficientNetB0 for Classification) and see which type works the best. \n\nThis project will contain 2 main sections. Alzheimer Detection and Alzheimer Classification. Detection models (PCA, SVM, LDA, VGG16) will be making judgements about the test data to see whether this person's brain image shows Alzheimer signs. \n\nOn the other hand, Alzheimer Classification will be done through training the data with different severity for Alzheimer. Actually, this is how the original data is made, and this model can be used as a detector as well. As ML model will provide what stage the patient is, it may be used as a supportive tool for doctors to diagnose Alzheimer's Disease. It brings some light in the practical world as it may find early stages of AD, which can increase life expectancy as well as increased life quality with supportive treatments. \n\nSummary: I will try to implement diverse models for Alzheimer detection(non vs with alzheimer) as well as Alzheimer Classifier(non vs very mild vs mild vs moderate)","metadata":{"id":"bgdjNLkktwha"}},{"cell_type":"markdown","source":"Prepare the data in 2 different ways. \n1. Alzheimer Detection: Whether the patient has the alzhiemer or not (non vs all other categories) \n2. Alzheimer Classifier: Define what stage the patient is in the alzheimer. \n\nThe models that are going to be tested here are \n1. PCA for Alzheimer Detection\n2. LDA for Alzheimer Detection\n3. SVM for Alzheimer Detection and Alzheimer Classifier\n4. CNN for Alzheimer Detection(VGG16) and Alzheimer Classifier(EfficientNetB0)","metadata":{"id":"Ht9wEuS6tuu1"}},{"cell_type":"code","source":"#Set up the environment and upload the data\nfrom sklearn.preprocessing import StandardScaler\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nimport pandas as pd\nfrom matplotlib.image import imread\nfrom skimage.io import imread_collection\nfrom PIL import Image\nimport seaborn as sns\nfrom sklearn import decomposition, preprocessing, svm\nimport sklearn.metrics as metrics #confusion_matrix, accuracy_score\nfrom time import sleep \nfrom tqdm.notebook import tqdm\nimport os\nsns.set()","metadata":{"id":"4bjWaHiAR73t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, download the data from Kaggle [https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset]","metadata":{"id":"hLFH9rl7sjZ7"}},{"cell_type":"code","source":"#Dataset that should go with Alzheimer label\nvery_mild = glob(r'C:\\Users\\green\\Desktop\\Dataset\\Very_Mild_Demented\\*')\nmild = glob(r'C:\\Users\\green\\Desktop\\Dataset\\Mild_Demented\\*')\nmoderate = glob(r'C:\\Users\\green\\Desktop\\Dataset\\Moderate_Demented\\*')\n\n#Dataset without Alzheimer\nnon = glob(r'C:\\Users\\green\\Desktop\\Dataset\\Non_Demented\\*')","metadata":{"id":"1KV3N8XQs6rW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(non[1])\ndef view_image(directory):\n    img = mpimg.imread(directory)\n    plt.imshow(img)\n    plt.title(directory)\n    plt.axis('off')\n    print(f'Image shape:{img.shape}')\n    return img\n\nprint('One of the data in Non Alzheimer Folder')\nview_image(non[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Alzheimer Patient\\'s Brain')\nview_image(moderate[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Testing and Analysis\n## 3.1 Alzheimer Detection Models\n### PCA for Alzheimer Detection","metadata":{"id":"ujKbEF9du3Yx"}},{"cell_type":"code","source":"def extract_feature(dir_path):\n    img = mpimg.imread(dir_path)\n    img = img / 255.0  # normalize pixel values\n    img = resize(img, (128, 128, 3))  # convert all images to (128x128x3)\n    img = np.reshape(img, (128, 384))\n    return img\n\nnon_ALZ = [extract_feature(filename) for filename in non]\nvmild_ALZ = [extract_feature(filename) for filename in very_mild]\nmild_ALZ = [extract_feature(filename) for filename in mild]\nmoderate_ALZ = [extract_feature(filename) for filename in moderate]\nALZ = vmild_ALZ + mild_ALZ + moderate_ALZ\n\n#for PCA\nall_data = np.concatenate((np.array(non_ALZ),np.array(ALZ)))\n#print(all_data)\nall_data = all_data.reshape(all_data.shape[0], np.product(all_data.shape[1:]))\n\nscaler = StandardScaler()\nscaler.fit(all_data)\n\n#standardize data to 0 mean and unit variance\nX = scaler.transform(all_data)\n\n#split the data \nfrom sklearn.model_selection import train_test_split\ny = [0] * len(non_ALZ) + [1] * len(ALZ)\nX_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.2)\n\nscala = preprocessing.StandardScaler()\n#Compressing the images into two dimensions using PCA\npca = decomposition.PCA(200)\nX_proj = pca.fit_transform(X_train)\n\n#let's first see which principal component works better\n#scree plot but cumulative\n# Getting the cumulative variance \nvar_cumu = np.cumsum(pca.explained_variance_ratio_)*100 #100 is multiplied for percentage\n \n# How many PCs explain 90% of the variance?\nk = np.argmax(var_cumu>80)\nprint(\"Number of components explaining 80% variance: \"+ str(k)) #I guess, I will have to use 160 axis\n#print(\"\\n\")\n \nplt.figure(figsize=[10,5])\nplt.title('Cumulative Explained Variance explained by the components')\nplt.ylabel('Cumulative Explained variance')\nplt.xlabel('Principal components')\nplt.axvline(x=k, color=\"k\", linestyle=\"--\")\nplt.axhline(y=80, color=\"r\", linestyle=\"--\")\nax = plt.plot(var_cumu)\n\nprint(X_proj)","metadata":{"id":"oe1rllZMu6Nk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From looking at the above, PCA will differentiate the dataset into two, a brain suffering from alzheimer or not. However, to have at least 80% accuracy of differentiation, it requires 174 principal components. This means that to differentiate the data with 80% accuracy, we will need to make a model that has 174 different axis in the data.","metadata":{"id":"WP2HOnxw1yfy"}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components=1)\nX_train_LDA = lda.fit_transform(X_train, y_train)\nX_test_LDA = lda.transform(X_test)\naccuracy = lda.score(X_test, y_test)\nprint(accuracy*100, '% accuracy (testing data)' )\naccuracy_train = lda.score(X_train, y_train)\nprint(accuracy_train*100, '% accuracy (training data)')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First time when I saw the performance of the LDA(Linear Discriminant Analysis) model, I was suprised to see the performance of the model on the training data is so high. as well as the performance on the testing data is also significant. This gave me a lot of confidence about the data as it seems to be very well taken with MRI. ","metadata":{}},{"cell_type":"markdown","source":"### SVM for Alzheimer Detection","metadata":{"id":"Pd3HEIZqu6hw"}},{"cell_type":"code","source":"#List where arrays shall be stored\nresized_image_array=[]\n#List that will store the answer if an image is female (0) or male (1)\nresized_image_array_label=[]\n\nwidth = 256\nheight = 256\nnew_size = (width,height) #the data is just black to white \n\n#Iterate over pictures and resize them to 256 by 256\ndef resizer(image_directory):\n    for file in image_directory: #tried with os.listdir but could work with os.walk as well\n        img = Image.open(file) #just putting image_directory or file does not work for google colab, interesting. \n        #preserve aspect ratio\n        img = img.resize(new_size)\n        array_temp = np.array(img)\n        shape_new = width*height\n        img_wide = array_temp.reshape(1, shape_new)\n        resized_image_array.append(img_wide[0])\n        if image_directory == non:\n            resized_image_array_label.append(0)\n        else:\n            resized_image_array_label.append(1)\n\nALZ = very_mild + mild + moderate\nresizer(non)\nresizer(ALZ)\n\n","metadata":{"id":"6BXX3YOltp5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(non))\nprint(len(ALZ)) #data are well transformed. Let's conduct SVM\nprint(len(resized_image_array))\nprint(resized_image_array[1])\n\n#split the data to test and training\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(resized_image_array, resized_image_array_label, test_size = 0.2)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train SVM model\n#from sklearn import svm\nclf = svm.SVC(kernel = 'linear')\nclf.fit(train_x, train_y)\n#store predictions and ground truth\ny_pred = clf.predict(train_x)\ny_true = train_y\n\n#assess the performance of the SVM with linear kernel on Training data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\nprint('Precision : ', metrics.precision_score(y_true, y_pred))\nprint('Recall : ', metrics.recall_score(y_true, y_pred))\nprint('f1 : ', metrics.f1_score(y_true, y_pred)) \nprint('Confusion matrix :', metrics.confusion_matrix(y_true, y_pred)) #The training seems to be done with high accuracy on training data.\n\n#Now, use the SVM model to predict Test data\ny_pred = clf.predict(test_x)\ny_true = test_y\n\n#assess the performance of the SVM with linear kernel on Testing data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\nprint('Precision : ', metrics.precision_score(y_true, y_pred))\nprint('Recall : ', metrics.recall_score(y_true, y_pred))\nprint('f1 : ', metrics.f1_score(y_true, y_pred)) \nprint('Confusion matrix :', metrics.confusion_matrix(y_true, y_pred)) #Having high training data accuracy might mean that it is having some overfitting","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train a SVM using polynomial kernel with degree of 2\nclf = svm.SVC(kernel = 'poly', degree = 2)\nclf.fit(train_x, train_y)\n\n#store predictions and ground truth\ny_pred = clf.predict(train_x)\ny_true = train_y\n\n#assess the performance of the SVM with linear kernel on Training data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\nprint('Precision : ', metrics.precision_score(y_true, y_pred))\nprint('Recall : ', metrics.recall_score(y_true, y_pred))\nprint('f1 : ', metrics.f1_score(y_true, y_pred)) \nprint('Confusion matrix :', metrics.confusion_matrix(y_true, y_pred))\n\n#Now, use the SVM model to predict Test data\ny_pred = clf.predict(test_x)\ny_true = test_y\n\n#assess the performance of the SVM with linear kernel on Testing data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\nprint('Precision : ', metrics.precision_score(y_true, y_pred))\nprint('Recall : ', metrics.recall_score(y_true, y_pred))\nprint('f1 : ', metrics.f1_score(y_true, y_pred)) \nprint('Confusion matrix :', metrics.confusion_matrix(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train a SVM using RBF kernel\nclf = svm.SVC(kernel = 'rbf')\nclf.fit(train_x, train_y)\n\n#store predictions and ground truth\ny_pred = clf.predict(train_x)\ny_true = train_y\n\n#assess the performance of the SVM with linear kernel on Training data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\nprint('Precision : ', metrics.precision_score(y_true, y_pred))\nprint('Recall : ', metrics.recall_score(y_true, y_pred))\nprint('f1 : ', metrics.f1_score(y_true, y_pred)) \nprint('Confusion matrix :', metrics.confusion_matrix(y_true, y_pred))\n\n#Now, use the SVM model to predict Test data\ny_pred = clf.predict(test_x)\ny_true = test_y\n\n#assess the performance of the SVM with linear kernel on Testing data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\nprint('Precision : ', metrics.precision_score(y_true, y_pred))\nprint('Recall : ', metrics.recall_score(y_true, y_pred))\nprint('f1 : ', metrics.f1_score(y_true, y_pred)) \nprint('Confusion matrix :', metrics.confusion_matrix(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Already from the SVM's linear kernel, the performance of the model is very promising. Considering that this model is not those of so-what-called 'State-of-the-Art', it still has a stellar performance. \n\nSuprisingly, as the kernel gets more complex, the overall performance does not necessarily rises. This may be the reason that 'having Alzheimer' or not is very easy to distinguish by looking at the thickness of the grey mater and the size of the ventricles. Which does not require some complex kernel tricks to be made. \n\nHowever, at this point, it is just 'Alzheimer detector' not 'Alzheimer classifier' as the original data is categorical variable. So, when I start conducting the Alzheimer classifier, more complex kernel may be better rather than a simple linear kernel trick.","metadata":{"id":"OXPJQPqP1wuK"}},{"cell_type":"markdown","source":"### VGG16 (CNN) for Alzheimer Detection\nVGG16 is known to take RGB colored data as the input for the model. However, the MRI images are in greyscale. So, I had to research on how to make the grayscale data to multi-channel before I feed it to VGG16. \nReference for this greyscale data : \n1. https://forums.fast.ai/t/black-and-white-images-on-vgg16/2479 \n2. https://forums.fast.ai/t/using-vgg-with-greyscale-images/854\n3. https://github.com/keras-team/keras/issues/11208","metadata":{}},{"cell_type":"code","source":"#pip install opencv-python","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This code cell is a revised version of Code snippet for pre-processing mnist data:https://github.com/keras-team/keras/issues/11208\n#The above code is revised for this project's purpose. \nimport cv2\ndim = (128, 128) #original shape of the image\n\ndef extract_feature(dir_path):\n    img = mpimg.imread(dir_path)\n    img = img / 255.0  # normalize pixel values\n    img = resize(img, (128, 128, 3))  # convert all images to (128x128x3)\n    img = np.reshape(img, (128, 384))\n    return img\n\nnon_ALZ = [extract_feature(filename) for filename in non]\nvmild_ALZ = [extract_feature(filename) for filename in very_mild]\nmild_ALZ = [extract_feature(filename) for filename in mild]\nmoderate_ALZ = [extract_feature(filename) for filename in moderate]\nALZ = vmild_ALZ + mild_ALZ + moderate_ALZ\n\n#for PCA\nall_data = np.concatenate((np.array(non_ALZ),np.array(ALZ)))\n#print(all_data)\nall_data = all_data.reshape(all_data.shape[0], np.product(all_data.shape[1:]))\n\nscaler = StandardScaler()\nscaler.fit(all_data)\n\n#standardize data to 0 mean and unit variance\nX = scaler.transform(all_data)\n\n#split the data \nfrom sklearn.model_selection import train_test_split\ny = [0] * len(non_ALZ) + [1] * len(ALZ)\nX_train, X_test, y_train, y_test = train_test_split(all_data, y, test_size=0.2)\n\n#convert grayscale to 128x128 rgb channels\ndef to_rgb(img):\n    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA) \n    img_rgb = np.asarray(np.dstack((img, img, img)), dtype=np.uint8)\n    return img_rgb\n\nrgb_list = []\n#convert X_train data to 128x128 rgb values\nfor i in range(len(X_train)):\n    rgb = to_rgb(X_train[i])\n    rgb_list.append(rgb)\n    #print(rgb.shape)\n    \nrgb_arr = np.stack([rgb_list],axis=4)\nrgb_arr_to_3d = np.squeeze(rgb_arr, axis=4)\nprint(rgb_arr_to_3d.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nrgb_list_test = []\n#convert X_test data to 128x128 rgb values\nfor i in range(len(X_test)):\n    rgb = to_rgb(X_test[i])\n    rgb_list_test.append(rgb)\n    #print(rgb.shape)\n    \nrgb_arr_test = np.stack([rgb_list_test],axis=4)\nrgb_arr_to_3d_test = np.squeeze(rgb_arr_test, axis=4)\nprint(rgb_arr_to_3d_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set up the environment\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Flatten, Dense, Input, Dropout\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing import image\n\nvgg16 = VGG16(weights = 'imagenet', input_shape = (128, 128, 3), include_top = False)\nvgg16.summary()\n\nvgg16.trainable = False\nfinal_model = Sequential()\nfinal_model.add(vgg16)\nfinal_model.add(Flatten(name='flatten'))\nfinal_model.add(Dense(2,activation = 'softmax', name = 'predictions'))\nfinal_model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nfinal_model.fit(rgb_arr_to_3d, np.array(y_train).reshape(-1,1), batch_size = 7, epochs = 10, verbose = 1)\nfinal_model.fit(rgb_arr_to_3d_test, np.array(y_test).reshape(-1,1), batch_size = 7, epochs = 10, verbose = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the results, SVM with linear kernel has 98% for all performance measure on testing data. Other kernels that are more complex than this did not work well with SVM as the linear kernel. LDA on the other hand had 90% accuracy on testing data. PCA can have 80% accuracy by having 174 principle components(may change as it depends how training and testing data are split). CNN model(VGG16) suprisingly had low performance considering that it is the most complicated model that I've used for the Alzheimer Detector. \n\nThis may be because that I've changed the greyscale to RBG(colored images) so that it fits the requirement for VGG16. (VGG16 is used for color images). Thus, making the data change might have affected the performance of VGG16. Accuracy of somewhere around 50% just means that it is not different from making random guess as there are only two choices for the AI model to decide. 'This image is a Alzheimer patient's brain' or not. \n\nTo see if this is the issue for CNN itself or from transforming the data, I will try different model in the Alzheimer Classifier. (EfficientNetB0)\n\n## 3.2 Alzheimer Classifier\nNow is the time where we make SVM, LDA, and CNN models for Alzheimer classifier. \n### SVM for Alzheimer Classifier","metadata":{}},{"cell_type":"code","source":"#List where arrays shall be stored\nresized_image_array=[]\n#List that will store the answer if an image is female (0) or male (1)\nresized_image_array_label=[]\n\nwidth = 256\nheight = 256\nnew_size = (width,height) #the data is just black to white \n\n#Iterate over pictures and resize them to 256 by 256\ndef resizer(image_directory):\n    for file in image_directory: #tried with os.listdir but could work with os.walk as well\n        img = Image.open(file) #just putting image_directory or file does not work for google colab, interesting. \n        #preserve aspect ratio\n        img = img.resize(new_size)\n        array_temp = np.array(img)\n        shape_new = width*height\n        img_wide = array_temp.reshape(1, shape_new)\n        resized_image_array.append(img_wide[0])\n        if image_directory == non:\n            resized_image_array_label.append(0)\n        elif image_directory == very_mild:\n            resized_image_array_label.append(1)\n        elif image_directory == mild:\n            resized_image_array_label.append(2)\n        else:\n            resized_image_array_label.append(3)\n\nresizer(non)\nresizer(very_mild)\nresizer(mild)\nresizer(moderate)\n\n#split the data to test and training\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(resized_image_array, resized_image_array_label, test_size = 0.2)\n\n#train SVM model\n#from sklearn import svm\nclf = svm.SVC(kernel = 'linear')\nclf.fit(train_x, train_y)\n#store predictions and ground truth\ny_pred = clf.predict(train_x)\ny_true = train_y\n\n#assess the performance of the SVM with linear kernel on Training data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\n\n#Now, use the SVM model to predict Test data\ny_pred = clf.predict(test_x)\ny_true = test_y\n\n#assess the performance of the SVM with linear kernel on Testing data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train a SVM using polynomial kernel with degree of 2\nclf = svm.SVC(kernel = 'poly', degree = 2)\nclf.fit(train_x, train_y)\n\n#store predictions and ground truth\ny_pred = clf.predict(train_x)\ny_true = train_y\n\n#assess the performance of the SVM with linear kernel on Training data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\n\n#Now, use the SVM model to predict Test data\ny_pred = clf.predict(test_x)\ny_true = test_y\n\n#assess the performance of the SVM with linear kernel on Testing data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train a SVM using RBF kernel\nclf = svm.SVC(kernel = 'rbf')\nclf.fit(train_x, train_y)\n\n#store predictions and ground truth\ny_pred = clf.predict(train_x)\ny_true = train_y\n\n#assess the performance of the SVM with linear kernel on Training data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))\n\n#Now, use the SVM model to predict Test data\ny_pred = clf.predict(test_x)\ny_true = test_y\n\n#assess the performance of the SVM with linear kernel on Testing data\nprint('Accuracy : ', metrics.accuracy_score(y_true, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above performance tells that the linear kernel still works the best for the classification as well compared to other complex kernels. Considering that kernel are used to separate the data with it's shape, the data seems to be very well separated linearly in the upper dimention. Also, the accuracy of the linear kernel SVM model is 98.67% which is higher than just Alzheimer detection model from above. \n\n\n### CNN model for Alzheimer Classifier (EfficientNetB0)\nRather than using the typical and basic models, here, I use a bit more state-of-the-art model EfficientNet. Depending on the input data's shape, EfficientNet changes a little bit. However, for the smallest shape (usual input is 224x224) EfficientNetB0 is used. \n\n- Reference \n1. https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n2. https://towardsdatascience.com/neural-architecture-transfer-54226b2306e3 \n3. https://towardsdatascience.com/state-of-the-art-image-classification-algorithm-fixefficientnet-l2-98b93deeb04c \n\n- First, see if the existing code in Kaggle is actually giving 99% accuracy with different seed. ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n#syntax from https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory\ndata = r'C:\\Users\\green\\Desktop\\Dataset'\ntrain_data = tf.keras.preprocessing.image_dataset_from_directory(\n    data,\n    labels = 'inferred',\n    label_mode='categorical',\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    color_mode='grayscale',\n    image_size=(128,128),\n    batch_size=32,\n)\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(\n    data,\n    labels = 'inferred',\n    label_mode='categorical',\n    validation_split= 0.2,\n    subset=\"validation\",\n    seed=42,\n    color_mode= 'grayscale',\n    image_size = (128,128),\n    batch_size=32,\n)\n\n#Model setup\nEffNet = tf.keras.applications.EfficientNetB0(include_top = False)\nEffNet.trainable = True\ninputs = tf.keras.layers.Input(shape=((128,128) + (1,)), name = 'input_layer')\nx = EffNet(inputs)\nx = tf.keras.layers.GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\noutputs = tf.keras.layers.Dense(4, activation = 'softmax', name='output_layer')(x)\nmodel = tf.keras.Model(inputs, outputs)\nLearning_Rate = 0.1\nmodel.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = Learning_Rate), metrics=['accuracy'])\nmodel.summary()\n\n#Training the Model\ngraphs = model.fit(train_data, validation_data = test_data, epochs = 10, verbose = False)\npd.DataFrame(graphs.history).plot(figsize=(8,6))\n\n#check the performance\nmodel.evaluate(test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before I make judgements, I'll try with smaller learning rate. ","metadata":{}},{"cell_type":"code","source":"#Model Changing the learning rate\nEffNet = tf.keras.applications.EfficientNetB0(include_top = False)\nEffNet.trainable = True\ninputs = tf.keras.layers.Input(shape=((128,128) + (1,)), name = 'input_layer')\nx = EffNet(inputs)\nx = tf.keras.layers.GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\noutputs = tf.keras.layers.Dense(4, activation = 'softmax', name='output_layer')(x)\nmodel = tf.keras.Model(inputs, outputs)\nLearning_Rate = 0.01\nmodel.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = Learning_Rate), metrics=['accuracy'])\nmodel.summary()\n\n#Training the Model\ngraphs = model.fit(train_data, validation_data = test_data, epochs = 10, verbose = False)\npd.DataFrame(graphs.history).plot(figsize=(8,6))\n\n#check the performance\nmodel.evaluate(test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above, the performance of the existing code(https://www.kaggle.com/code/gonzalorecioc/alzheimer-brain-mri-classifier-effnetb0-99-acc) had 99% accuracy. But actually, having a different seed, the accuracy drops significantly to 58% with learning rate of 0.1 and 85% with learning rate of 0.01. No other fundamental changes in the code exists when it comes to learning rate of 0.01. \n\nHowever, since there was 99% accuracy, there should be a way to boost the accuracy to at least 90%. The reason for having low accuracy may be the issue that the shape of the given data input is 128x128 rather than 224x224, which is the input that is expected for EfficientNetB0. \n\nSo, in below, I will resize the image and try running CNN again with the changed size. \n","metadata":{}},{"cell_type":"code","source":"training_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    directory,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    class_names=None,\n    color_mode=\"grayscale\",\n    batch_size=32,\n    image_size=(224, 224),\n    shuffle=True,\n    seed=123,\n    validation_split=0.2,\n    subset=\"training\",\n    interpolation=\"bilinear\",\n    follow_links=False,\n    crop_to_aspect_ratio=True\n)\n\ntesting_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    directory,\n    labels=\"inferred\",\n    label_mode=\"categorical\",\n    class_names=None,\n    color_mode=\"grayscale\",\n    batch_size=32,\n    image_size=(224, 224),\n    shuffle=True,\n    seed=123,\n    validation_split=0.2,\n    subset=\"validation\",\n    interpolation=\"bilinear\",\n    follow_links=False,\n    crop_to_aspect_ratio=True\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model setup\nEffNet = tf.keras.applications.EfficientNetB0(include_top = False)\nEffNet.trainable = True\ninputs = tf.keras.layers.Input(shape=((224,224) + (1,)), name = 'input_layer')\nx = EffNet(inputs)\nx = tf.keras.layers.GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\noutputs = tf.keras.layers.Dense(4, activation = 'softmax', name='output_layer')(x)\nmodel = tf.keras.Model(inputs, outputs)\nLearning_Rate = 0.01\nmodel.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate = Learning_Rate), metrics=['accuracy'])\nmodel.summary()\n\n#Training the Model\ngraphs = model.fit(training_dataset, validation_data = testing_dataset, epochs = 10, verbose = False)\npd.DataFrame(graphs.history).plot(figsize=(8,6))\n\n#check the performance\nmodel.evaluate(testing_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, from the performance, this data really seems to fit well on SVM, which runs way faster than CNN. LDA also had a high accuracy as well on alzheimer detection. But considering the nature of LDA, having to see that the data is linearly separable from SVM, it shouldn't be that different with LDA as it separates data linearly as well. \n\nMost suprising result was tho that CNN did not work that well on my end with the high learning rate, while it does seem to have had high results in Kaggle. I don't know what is the difference in fundamental, however, the result of my CNN models including VGG16 and EfficientNetB0 sometimes were trained to have lower accuracy even compared to random guessing. This part of the project was interesting to learn why 'right sized learning rate' is important overall as it increased the accuracy significantly by more than 30%p comapred to those with bigger learning rates. \n\n### Overall Summary \nHaving 80% accuracy and such performance on testing data is already 'good', however, in this project, there were many models that went over 90%. Here is the overall results of the model and its' performance. I highlighted the results with over 90% performance with HTML tag.\n\n| Project | Models | Performance on test data|\n| :------- | :------ | :----------- |\n| Alzheimer Detection | PCA | 174 axis needed to reach 80% accuracy |\n| \" | SVM(Kernel:Linear) | <mark> 98% accuracy and f1 score </mark> |\n| \" | SVM(Kernel: 2-degree polynomial) | 86% accuracy and f1 score |\n| \" | SVM(Kernel:RBF) | 83% accuracy and f1 score | \n| \" | LDA | <mark>90.1% accuracy</mark> |\n| \" | CNN(VGG16) | 50% accuracy|\n| Alzheimer Classification | SVM(Kernel:Linear) | <mark>98.7% accuracy</mark> |\n| \" | SVM(Kernel:2-degree polynomial) | 83.1% accuracy |\n| \" | SVM(Kernel: RBF) | 77% accuracy |\n| \" | CNN(EffnetB0) - LR:0.1 | 58% accuracy |\n| \" | CNN(EffnetB0) - LR:0.01 | 85% accuracy |\n| \" | CNN(EfficientnetB0) - Resize | <mark>83% accuracy</mark> |\n\nHaving the accuracy like 90% or more may mean that they are better than human, depending on the context. And it was fun to try diverse models and improving the accuracy of ML model. I kind of got carried away with trying diverse methods :)\n\n## 4. References \nAlzheimer mri preprocessed dataset. (n.d.). Retrieved from https://www.kaggle.com/sachinkumar413/alzheimer-mri-dataset\n\nBlack and white images on VGG16. (2017). Deep Learning Course Forums. https://forums.fast.ai/t/black-and-white-images-on-vgg16/2479\n\nCarpenter, A. (2020,). Neural architecture transfer. Medium. https://towardsdatascience.com/neural-architecture-transfer-54226b2306e3\n\nMarius, H. (2021). State-of-the-art image classification algorithm: Fixefficientnet-l2. Medium. https://towardsdatascience.com/state-of-the-art-image-classification-algorithm-fixefficientnet-l2-98b93deeb04c\n\nTeam, K. (n.d.). Keras documentation: Image classification via fine-tuning with EfficientNet. Retrieved from https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n\nTransfer Learning(Vgg16) using MNIST · Issue #11208 · keras-team/keras. (n.d.). GitHub. Retrieved from https://github.com/keras-team/keras/issues/11208\n\nUsing vgg with greyscale images. (2017). Deep Learning Course Forums. https://forums.fast.ai/t/using-vgg-with-greyscale-images/854\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}